{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884df549",
   "metadata": {},
   "source": [
    "# UNET Class and other pre-requisites to load the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d32821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn #imports necessary neural network toolbox\n",
    "import torch.nn.functional as nnfunc \n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Double Convolution Block - Basic building block of U-Net\n",
    "    Pattern: Conv -> BatchNorm -> ReLU -> Conv -> BatchNorm -> ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),#the first convolution of the double conv.\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # batch norm will help normalise the effects of activation functions\n",
    "            # so that the model will be more stable and fast as per Geek4Geeks.\n",
    "            nn.ReLU(inplace=True), # <- the activation function\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), #second convolution\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        This confused me initially, I thought that the code under def__init__\n",
    "        would be enough, but according to Geeks4Geeks: \n",
    "        https://www.geeksforgeeks.org/deep-learning/understanding-the-forward-function-output-in-pytorch/\n",
    "        \n",
    "        The forward method is PyTorch's implementation of a forward pass:\n",
    "        that's when the data/tensor is passed through the model\n",
    "        \"\"\"\n",
    "        return self.double_conv(tensor)\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net Architecture for Medical Image Segmentation\n",
    "    encoder (Downsampling):  Input -> Conv -> Pool -> Conv -> Pool -> bottleneck\n",
    "    decoder (Upsampling):    bottleneck -> Upsample -> Concat -> Conv -> ... -> Output\n",
    "    skip connections: Connect encoder features directly to decoder for fine details\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNET, self).__init__() \n",
    "        \"\"\"\n",
    "        A super().__init__() is necessary to initialise the base nn.Module \n",
    "        so that dependencies and everything works for neural networks in PyTorch\n",
    "        \n",
    "        Args:\n",
    "            in_channels: number of input channels, 1 for grayscale\n",
    "            out_channels: number of output channels, 1 for binary segmentation\n",
    "            \n",
    "        The U-Net paper has the following encoder implementation:\n",
    "        1) Double conv (64 channels)\n",
    "        2) Max pool 2x2 + Double conv (128 chnnels)  \n",
    "        3) Max pool 2x2 + Double conv (256 chanels)\n",
    "        4) Max pool 2x2 + Double conv (512 channls)\n",
    "        5) Max pool 2x2 + Double conv (1024 chanels)\n",
    "        \n",
    "        Using this, there are 5 different double conv operations\n",
    "        but we reuse the same max pool operation because the double conv\n",
    "        has to account for changes in channel dimensions\n",
    "        \"\"\"\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)  # halves spatial dimensions\n",
    "        \n",
    "        # encoder path (Contracting/Downsampling)\n",
    "        # Each step: DoubleConv -> MaxPool (except bottleneck)\n",
    "        self.dconv1 = DoubleConv(in_channels, 64)   # Input -> 64 channes   (128x128 -> 128x128)\n",
    "        self.dconv2 = DoubleConv(64, 128)           # 64 -> 128 channels     (64x64 -> 64x64)\n",
    "        self.dconv3 = DoubleConv(128, 256)          # 128 -> 256 chanels    (32x32 -> 32x32)\n",
    "        self.dconv4 = DoubleConv(256, 512)          # 256 -> 512 chanels    (16x16 -> 16x16)\n",
    "        self.dconv5 = DoubleConv(512, 1024)         # 512 -> 1024 chanels   (8x8 -> 8x8) bottleneck\n",
    "        \n",
    "        # decoder path (Expanding/Upsampling)\n",
    "        # these are the variables for the decoder - upsampling is the opposite of max pooling\n",
    "        \n",
    "        # Level 4: from bottleneck (1024 ch, 8x8) -> Level 4 encoder (512 ch, 16x16)\n",
    "        self.upsample4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)  # 8x8 -> 16x16\n",
    "        self.upconv4 = DoubleConv(1024, 512)  # 1024 channels (512+512 from skip) -> 512\n",
    "        \n",
    "        # Level 3: from level 4 (512 ch, 16x16) -> Level 3 encoder (256 ch, 32x32)  \n",
    "        self.upsample3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)   # 16x16 -> 32x32\n",
    "        self.upconv3 = DoubleConv(512, 256)   # 512 channels (256+256 from skip) -> 256\n",
    "        \n",
    "        # Level 2: from level 3 (256 ch, 32x32) -> Level 2 encoder (128 ch, 64x64)\n",
    "        self.upsample2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)   # 32x32 -> 64x64\n",
    "        self.upconv2 = DoubleConv(256, 128)   # 256 channels (128+128 from skip) -> 128\n",
    "        \n",
    "        # Level 1: from level 2 (128 ch, 64x64) -> Level 1 encoder (64 ch, 128x128)\n",
    "        self.upsample1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)    # 64x64 -> 128x128\n",
    "        self.upconv1 = DoubleConv(128, 64)    # 128 channels (64+64 from skip) -> 64\n",
    "        \n",
    "        # OUTPUT LAYER\n",
    "        # after this the last step is one final 1x1 conv layer\n",
    "        self.outconv = nn.Conv2d(64, 1, kernel_size=1)  # 64 -> 1 channel (segmentation mask)\n",
    "    \n",
    "    def _concat(self, up_tensor, skip_tensor):\n",
    "        \"\"\"\n",
    "        Concatenate upsampled tensor with skip connection from encoder.\n",
    "        handle size mismatches by padding the smaller tensor.\n",
    "        \n",
    "        Skip connections are crucial because they:\n",
    "        1. Preserve fine spatial details lost during downsampling\n",
    "        2. Help gradients flow better during backpropagation  \n",
    "        3. Combine low-level features (edges) with high-level features (semantics)\n",
    "        \"\"\"\n",
    "        # Check if spatial dimensions match between upsampled and skip tensors\n",
    "        if up_tensor.size(2) != skip_tensor.size(2) or up_tensor.size(3) != skip_tensor.size(3):\n",
    "            # Calculate differences in Height and Width\n",
    "            y_ax_diff = skip_tensor.size(2) - up_tensor.size(2)  # height difference\n",
    "            x_ax_diff = skip_tensor.size(3) - up_tensor.size(3)  # width difference\n",
    "\n",
    "            # Pad order in F.pad = [left, right, top, bottom]\n",
    "            # Distribute padding evenly, with extra pixels on right/bottom if odd\n",
    "            up_tensor = nnfunc.pad(\n",
    "                up_tensor,\n",
    "                [x_ax_diff // 2, x_ax_diff - x_ax_diff // 2,   # [left, right]\n",
    "                 y_ax_diff // 2, y_ax_diff - y_ax_diff // 2]   # [top, bottom]\n",
    "            )\n",
    "        \n",
    "        # concatenate along channel dimension (dim=1)\n",
    "        # noting that tensor format: [batch, channel, height, width]\n",
    "        # result: channels = skip_channels + up_channels\n",
    "        return torch.cat([skip_tensor, up_tensor], dim=1)\n",
    "    \n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        Forward pass through U-Net\n",
    "        \n",
    "        Data Flow:\n",
    "        Input -> encoder (extract features) -> bottleneck -> decoder (reconstruct) -> Output\n",
    "        \n",
    "        Skip connections preserve spatial information lost during downsampling\n",
    "        \n",
    "        input tensor shape: (batch_size, channels, height, width)\n",
    "        echocardiograms frames: (batch_size, 1, 112, 112)\n",
    "        \"\"\"\n",
    "        \n",
    "        # encoder path (steps 1-9) - Extract features at multiple scales\n",
    "        # save encoder outputs for skip connections (step1, step3, step5, step7)\n",
    "        \n",
    "        \n",
    "        step1 = self.dconv1(tensor)           # Input(B,1,112,112) -> (B,64,112,112)\n",
    "        step2 = self.max_pool(step1)          # (B,64,112,112) -> (B,64,56,56)\n",
    "        \n",
    "        step3 = self.dconv2(step2)            # (B,64,56,56) -> (B,128,56,56)  \n",
    "        step4 = self.max_pool(step3)          # (B,128,56,56) -> (B,128,28,28)\n",
    "        \n",
    "        step5 = self.dconv3(step4)            # (B,128,28,28) -> (B,256,28,28)\n",
    "        step6 = self.max_pool(step5)          # (B,256,28,28) -> (B,256,14,14)\n",
    "        \n",
    "        step7 = self.dconv4(step6)            # (B,256,14,14) -> (B,512,14,14)\n",
    "        step8 = self.max_pool(step7)          # (B,512,14,14) -> (B,512,7,7)\n",
    "        \n",
    "        step9 = self.dconv5(step8)            # (B,512,7,7) -> (B,1024,7,7) bottleneck\n",
    "        \n",
    "        # decoder PATH - Reconstruct spatial resolution with skip connections\n",
    "        # Each step: Upsample -> Concatenate with encoder -> Process with DoubleConv\n",
    "        \n",
    "        # Level 4: Combine bottleneck with encoder level 4 (step7)\n",
    "        step10 = self.upsample4(step9)        # (B,1024,7,7) -> (B,512,14,14)\n",
    "        step11 = self._concat(step10, step7)  # Concat: (B,512,14,14) + (B,512,14,14) -> (B,1024,14,14)\n",
    "        step12 = self.upconv4(step11)         # (B,1024,14,14) -> (B,512,14,14)\n",
    "\n",
    "        # Level 3: Combine with encoder level 3 (step5) \n",
    "        step13 = self.upsample3(step12)       # (B,512,14,14) -> (B,256,28,28)\n",
    "        step14 = self._concat(step13, step5)  # Concat: (B,256,28,28) + (B,256,28,28) -> (B,512,28,28)\n",
    "        step15 = self.upconv3(step14)         # (B,512,28,28) -> (B,256,28,28)\n",
    "\n",
    "        # Level 2: Combine with encoder level 2 (step3)\n",
    "        step16 = self.upsample2(step15)       # (B,256,28,28) -> (B,128,56,56)\n",
    "        step17 = self._concat(step16, step3)  # Concat: (B,128,56,56) + (B,128,56,56) -> (B,256,56,56)\n",
    "        step18 = self.upconv2(step17)         # (B,256,56,56) -> (B,128,56,56)\n",
    "\n",
    "        # Level 1: Final upsampling to original resolution, combine with step1\n",
    "        step19 = self.upsample1(step18)       # (B,128,56,56) -> (B,64,112,112)\n",
    "        step20 = self._concat(step19, step1)  # Concat: (B,64,112,112) + (B,64,112,112) -> (B,128,112,112)\n",
    "        step21 = self.upconv1(step20)         # (B,128,112,112) -> (B,64,112,112)\n",
    "\n",
    "        # output - Map feature channels to segmentation mask\n",
    "        # Returns raw logits (before sigmoid) - loss function will handle this\n",
    "        return self.outconv(step21)           # (B,64,112,112) -> (B,1,112,112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f19a8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dice_coeff(predictions, targets, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    no dice loss function in pytorch\n",
    "    args:\n",
    "        predictions: Sigmoid probabilities [0,1] \n",
    "        targets: Binary ground truth [0,1]\n",
    "        smooth: Smoothing factor to avoid division by zero\n",
    "    returns:\n",
    "        dice_score: Float between 0 and 1\n",
    "    \"\"\"\n",
    "    # Flatten tensors for easier calculation\n",
    "    pred_flat = predictions.reshape(-1)\n",
    "    target_flat = targets.reshape(-1)\n",
    "    \n",
    "    # Calculate intersection (overlap)\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    \n",
    "    # Calculate union (total covered area)\n",
    "    union = pred_flat.sum() + target_flat.sum()\n",
    "    \n",
    "    # Dice coefficient: 2 * intersection / union\n",
    "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    return dice\n",
    "\n",
    "\n",
    "def CombinedBCEDiceLoss(predictions, targets, bce_weight=0.5, dice_weight=0.5):\n",
    "    '''\n",
    "    Combined BCE + Dice Loss for binary segmentation\n",
    "    args:\n",
    "        1.predictions (torch.Tensor): the raww model outputs (logits) from U-Net\n",
    "            - expecting shape: (batch_size, 1, height, width) \n",
    "            \n",
    "        2.targets (torch.Tensor): the ground truth binary masks\n",
    "            - shape: (batch_size, 1, height, width) - same as predictions\n",
    "            \n",
    "        3. bce_weight (float):weighting for the bce component, set to 0.5 for equal weight with dice\n",
    "            - BCE is good at: sharp boundaries, overall pixel classification\n",
    "            \n",
    "        4. dice_weight (float): weighting for the dice component, set to 0.5\n",
    "            - Dice is good at: handling class imbalance, shape preservation\n",
    "    \n",
    "    returns:\n",
    "        tuple: (combined_loss, bce_component, dice_component)\n",
    "            - combined_loss (torch.Tensor): Weighted sum for backpropagation\n",
    "            - bce_component (float): BCE loss value for monitoring\n",
    "            - dice_component (float): Dice loss value for monitoring\n",
    "    '''\n",
    "    # BCE part\n",
    "    bce_loss = nn.BCEWithLogitsLoss()(predictions, targets)\n",
    "    \n",
    "    # Dice part\n",
    "    predictions_sigmoid = torch.sigmoid(predictions)  #using sigmoid function to perform binary classification of the pixels\n",
    "    dice_score = dice_coeff(predictions_sigmoid, targets) #compares the predictions agains the ground truth\n",
    "    dice_loss = 1.0 - dice_score\n",
    "    \n",
    "    # Combined loss\n",
    "    combined_loss = (bce_weight * bce_loss) + (dice_weight * dice_loss)\n",
    "    \n",
    "    return combined_loss, bce_loss.item(), dice_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8793e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for computation (GPU if available, else CPU)\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Model First:\n",
    "def load_trained_model(path=\"best_unet_adamw_cosine.pt\"):  # Fixed typo in filename\n",
    "    \"\"\"Load the trained U-Net model from checkpoint\"\"\"\n",
    "    model = UNET(1, 1).to(device)\n",
    "    \n",
    "    if os.path.exists(path):  # Check if model exists\n",
    "        try:\n",
    "            checkpoint = torch.load(path, map_location=device, weights_only=False)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(\"Model loaded\")\n",
    "            model.eval()\n",
    "            return model, checkpoint\n",
    "        except Exception as e:\n",
    "            print(\"Model not loaded\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        return None, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b88c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "# Make sure to load the trained model first -- see code above\n",
    "# Train on the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def predict_one_folder(model, folder, threshold=0.5):\n",
    "    '''\n",
    "    Use this to predict 1 folder first to see the results, and prototype the video output\n",
    "\n",
    "    args: \n",
    "        model: UNET(in_chan, out_chan)\n",
    "        folder: the path to the folder for prediction-- use test folder instead of train/val\n",
    "        threshold: threshold for binary pred set to 0.5\n",
    "\n",
    "    returns: a dictionary called predictions\n",
    "        frame: framename relating to the predicted frame\n",
    "        frame_image: the original frame image\n",
    "        mask: binary mask as numpy arrays (just like how original segmentation mask was created)\n",
    "        pixel_count: pixel count for the mask\n",
    "    '''\n",
    "    folder = Path(folder)\n",
    "    frame_paths = sorted(folder.glob(\"*.png\")) # look for .png files in the specified folder\n",
    "\n",
    "    predictions = {\n",
    "        'frame': [],           # framename \n",
    "        'frame_image': [],     # frameimage\n",
    "        'mask': [],            # binary mask\n",
    "        'pixel_count': []      # pixel count for the mask\n",
    "    }\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad(): # disable backprop\n",
    "        for frame_path in frame_paths:\n",
    "            try:\n",
    "                # loading the original frame\n",
    "                frame = cv2.imread(str(frame_path), cv2.IMREAD_GRAYSCALE) # specifying that it's grayscale\n",
    "\n",
    "                # Normalise to [0,1] for gradient stability, and to help the sigmoid act function to work with the threshold of 0.5\n",
    "                frame_normalised = frame.astype(np.float32)/255.0\n",
    "                frame_tensor = torch.from_numpy(frame_normalised) # creates tensor shape (112, 112) corresp to H*W\n",
    "                frame_tensor = frame_tensor.unsqueeze(0) # add channel dim, = 1 so shape is (1,112,112)\n",
    "                frame_tensor = frame_tensor.unsqueeze(0) # add batch dim, so shape is (1,1,112,112)\n",
    "                frame_tensor = frame_tensor.to(device) # use GPU\n",
    "\n",
    "                # run the model to output logits which are just raw numbers\n",
    "                with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n",
    "                    logits = model(frame_tensor) # creates raw logits\n",
    "                    probs = torch.sigmoid(logits) # after sigmoid it changes this to [0,1]\n",
    "                    binary_mask = (probs > threshold).float() # apply the 0.5 threshold\n",
    "\n",
    "                mask_np = binary_mask.squeeze().cpu().numpy()  # Move to CPU first, then convert\n",
    "                pixel_count = int(np.sum(mask_np))\n",
    "\n",
    "                # store the result of each frame in frame_path in the dict.\n",
    "                predictions['frame'].append(frame_path.name)         # filename\n",
    "                predictions['frame_image'].append(frame)             # original image (112x112)\n",
    "                predictions['mask'].append(mask_np)                  # binary mask (112x112)\n",
    "                predictions['pixel_count'].append(pixel_count)       # pix count for later\n",
    "\n",
    "            except Exception:\n",
    "                print(\"error\")\n",
    "                break\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d126b28",
   "metadata": {},
   "source": [
    "# Code to Calculate LVEF per video here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0e78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "‚úÖ Model loaded successfully!\n",
      "Found 10030 video directories\n",
      "Processing ALL 10030 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  38%|‚ñà‚ñà‚ñà‚ñä      | 3787/10030 [2:24:57<3:21:32,  1.94s/it] "
     ]
    }
   ],
   "source": [
    "# Add missing imports\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. First, load your trained model\n",
    "loaded_model, checkpoint_info = load_trained_model(\"best_unet_adamw_cosine.pt\")\n",
    "\n",
    "def predict_one_fast(model, folder, threshold=0.5, batch_size=64):\n",
    "    \"\"\"\n",
    "    Fast batch processing without DataLoader workers (Windows-friendly)\n",
    "    \"\"\"\n",
    "    folder = Path(folder)\n",
    "    frame_paths = sorted(folder.glob(\"*.png\"))\n",
    "    \n",
    "    if not frame_paths:\n",
    "        return {'frame': [], 'frame_image': [], 'mask': [], 'pixel_count': []}\n",
    "    \n",
    "    predictions = {\n",
    "        'frame': [],\n",
    "        'frame_image': [],\n",
    "        'mask': [],\n",
    "        'pixel_count': []\n",
    "    }\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Process in batches manually (no DataLoader)\n",
    "        for i in range(0, len(frame_paths), batch_size):\n",
    "            batch_paths = frame_paths[i:i+batch_size]\n",
    "            batch_tensors = []\n",
    "            batch_frames = []\n",
    "            batch_names = []\n",
    "            \n",
    "            # Load batch of frames\n",
    "            for frame_path in batch_paths:\n",
    "                try:\n",
    "                    frame = cv2.imread(str(frame_path), cv2.IMREAD_GRAYSCALE)\n",
    "                    if frame is None:\n",
    "                        continue\n",
    "                        \n",
    "                    frame_normalised = frame.astype(np.float32) / 255.0\n",
    "                    frame_tensor = torch.from_numpy(frame_normalised).unsqueeze(0)  # Add channel dim\n",
    "                    \n",
    "                    batch_tensors.append(frame_tensor)\n",
    "                    batch_frames.append(frame)\n",
    "                    batch_names.append(frame_path.name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading frame {frame_path}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if not batch_tensors:\n",
    "                continue\n",
    "                \n",
    "            # Stack tensors and move to device\n",
    "            batch_tensor = torch.stack(batch_tensors).to(device)\n",
    "            \n",
    "            # Process batch through model\n",
    "            try:\n",
    "                with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n",
    "                    logits = model(batch_tensor)\n",
    "                    probs = torch.sigmoid(logits)\n",
    "                    binary_masks = (probs > threshold).float()\n",
    "                \n",
    "                # Move results back to CPU\n",
    "                binary_masks_cpu = binary_masks.squeeze(1).cpu().numpy()\n",
    "                \n",
    "                # Store results\n",
    "                for mask_np, frame_name, frame in zip(binary_masks_cpu, batch_names, batch_frames):\n",
    "                    pixel_count = int(np.sum(mask_np))\n",
    "                    \n",
    "                    predictions['frame'].append(frame_name)\n",
    "                    predictions['frame_image'].append(frame)\n",
    "                    predictions['mask'].append(mask_np)\n",
    "                    predictions['pixel_count'].append(pixel_count)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def calculate_lvef_from_predictions(predictions):\n",
    "    \"\"\"Calculate LVEF from pixel counts\"\"\"\n",
    "    if not predictions or len(predictions['pixel_count']) < 2:\n",
    "        return None\n",
    "    \n",
    "    pixel_counts = np.array(predictions['pixel_count'])\n",
    "    \n",
    "    esv_pixel = np.min(pixel_counts)\n",
    "    esv_frame_index = np.argmin(pixel_counts)\n",
    "    edv_pixel = np.max(pixel_counts)\n",
    "    edv_frame_index = np.argmax(pixel_counts)\n",
    "    \n",
    "    if edv_pixel > 0:\n",
    "        lvef_pixel = ((edv_pixel - esv_pixel) / edv_pixel) * 100\n",
    "    else:\n",
    "        lvef_pixel = 0\n",
    "    \n",
    "    return {\n",
    "        'ESV_pixel': int(esv_pixel),\n",
    "        'EDV_pixel': int(edv_pixel),\n",
    "        'LVEF_pixel': round(lvef_pixel, 2),\n",
    "        'ESV_frame': predictions['frame'][esv_frame_index],\n",
    "        'EDV_frame': predictions['frame'][edv_frame_index],\n",
    "        'total_frames': len(predictions['frame'])\n",
    "    }\n",
    "\n",
    "def process_multiple_videos_fast(video_directories, model, batch_size=16):\n",
    "    \"\"\"\n",
    "    Process multiple videos efficiently without DataLoader workers\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for video_dir in tqdm(video_directories, desc=\"Processing videos\"):\n",
    "        try:\n",
    "            # Use fast batch processing\n",
    "            predictions = predict_one_fast(model, video_dir, threshold=0.5, batch_size=batch_size)\n",
    "            \n",
    "            if not predictions or len(predictions['pixel_count']) == 0:\n",
    "                continue\n",
    "            \n",
    "            lvef_results = calculate_lvef_from_predictions(predictions)\n",
    "            if lvef_results is None:\n",
    "                continue\n",
    "            \n",
    "            video_id = Path(video_dir).name\n",
    "            result_row = {\n",
    "                'Key': video_id,\n",
    "                'Min_pixel': lvef_results['ESV_pixel'],\n",
    "                'Max_pixel': lvef_results['EDV_pixel'],\n",
    "                'EF_pixel': lvef_results['LVEF_pixel'],\n",
    "                'ESV_frame': lvef_results['ESV_frame'],\n",
    "                'EDV_frame': lvef_results['EDV_frame'],\n",
    "                'total_frames': lvef_results['total_frames']\n",
    "            }\n",
    "            results.append(result_row)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_dir}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def load_ground_truth_data(csv_path=\"FileList.csv\"):\n",
    "    try:\n",
    "        gt_df = pd.read_csv(csv_path)\n",
    "        gt_df['Key'] = gt_df['FileName'].str.replace('.avi', '').str.replace('.mp4', '')\n",
    "        return gt_df[['Key', 'EF', 'ESV', 'EDV']].copy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ground truth: {e}\")\n",
    "        return None\n",
    "\n",
    "def compare_with_ground_truth(predictions_df, ground_truth_df):\n",
    "    comparison_df = predictions_df.merge(ground_truth_df, on='Key', how='inner')\n",
    "    comparison_df['EF_error'] = comparison_df['EF_pixel'] - comparison_df['EF']\n",
    "    comparison_df['EF_error_abs'] = comparison_df['EF_error'].abs()\n",
    "    return comparison_df\n",
    "\n",
    "def quick_plot_results(comparison_df):\n",
    "    \"\"\"Simplified plotting\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(comparison_df['EF'], comparison_df['EF_pixel'], alpha=0.6)\n",
    "    axes[0].plot([0, 100], [0, 100], 'r--', label='Perfect Agreement')\n",
    "    axes[0].set_xlabel('Ground Truth EF (%)')\n",
    "    axes[0].set_ylabel('Predicted EF (%)')\n",
    "    axes[0].set_title('Predicted vs Ground Truth LVEF')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Error histogram\n",
    "    axes[1].hist(comparison_df['EF_error'], bins=15, alpha=0.7)\n",
    "    axes[1].axvline(0, color='red', linestyle='--')\n",
    "    axes[1].set_xlabel('EF Error (%)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Error Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = comparison_df['EF_error_abs'].mean()\n",
    "    rmse = np.sqrt((comparison_df['EF_error']**2).mean())\n",
    "    correlation = comparison_df['EF'].corr(comparison_df['EF_pixel'])\n",
    "    \n",
    "    print(f\"üìä Results: MAE={mae:.2f}%, RMSE={rmse:.2f}%, Correlation={correlation:.3f}\")\n",
    "    return mae, rmse, correlation\n",
    "\n",
    "# üöÄ MAIN EXECUTION (FIXED VERSION)\n",
    "# üöÄ MAIN EXECUTION (PROCESS ALL VIDEOS)\n",
    "if loaded_model is not None:\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "    # Get video directories\n",
    "    video_base_path = \"EchoNet-Dynamic/frame_outputs\"\n",
    "    test_video_dirs = [d for d in glob.glob(f\"{video_base_path}/*\") if os.path.isdir(d)]\n",
    "    \n",
    "    if test_video_dirs:\n",
    "        print(f\"Found {len(test_video_dirs)} video directories\")\n",
    "        \n",
    "        # Process ALL videos (removed the [:20] slice)\n",
    "        print(f\"Processing ALL {len(test_video_dirs)} videos...\")\n",
    "        \n",
    "        # Use fast processing (no DataLoader workers)\n",
    "        predictions_df = process_multiple_videos_fast(test_video_dirs, loaded_model, batch_size=8)\n",
    "        \n",
    "        if len(predictions_df) > 0:\n",
    "            print(f\"\\n‚úÖ Processed {len(predictions_df)} videos\")\n",
    "            print(\"\\nSample results:\")\n",
    "            print(predictions_df.head())\n",
    "            \n",
    "            # Quick statistics\n",
    "            print(f\"\\nüìä Quick Stats:\")\n",
    "            print(f\"   Average LVEF: {predictions_df['EF_pixel'].mean():.1f}%\")\n",
    "            print(f\"   LVEF Range: {predictions_df['EF_pixel'].min():.1f}% - {predictions_df['EF_pixel'].max():.1f}%\")\n",
    "            \n",
    "            # Save predictions\n",
    "            predictions_df.to_csv(\"lvef_predictions_all_videos.csv\", index=False)\n",
    "            print(\"üíæ Results saved to lvef_predictions_all_videos.csv\")\n",
    "            \n",
    "            # Load ground truth and compare\n",
    "            ground_truth_df = load_ground_truth_data(\"FileList.csv\")\n",
    "            if ground_truth_df is not None:\n",
    "                comparison_df = compare_with_ground_truth(predictions_df, ground_truth_df)\n",
    "                if len(comparison_df) > 0:\n",
    "                    print(f\"\\n‚úÖ Matched {len(comparison_df)} videos with ground truth\")\n",
    "                    \n",
    "                    # Quick visualization\n",
    "                    mae, rmse, correlation = quick_plot_results(comparison_df)\n",
    "                    \n",
    "                    # Save comparison results\n",
    "                    comparison_df.to_csv(\"lvef_comparison_all_videos.csv\", index=False)\n",
    "                    print(\"üíæ Comparison results saved to lvef_comparison_all_videos.csv\")\n",
    "                else:\n",
    "                    print(\"‚ùå No matching videos found with ground truth\")\n",
    "        else:\n",
    "            print(\"‚ùå No videos processed\")\n",
    "    else:\n",
    "        print(\"‚ùå No video directories found\")\n",
    "        print(\"Please check if EchoNet-Dynamic/frame_outputs/ exists\")\n",
    "else:\n",
    "    print(\"‚ùå Model not loaded\")\n",
    "# Quick test function\n",
    "def test_single_video():\n",
    "    \"\"\"Test on one video\"\"\"\n",
    "    if loaded_model is not None:\n",
    "        video_dirs = [d for d in glob.glob(\"EchoNet-Dynamic/frame_outputs/*\") if os.path.isdir(d)]\n",
    "        if video_dirs:\n",
    "            test_dir = video_dirs[0]\n",
    "            print(f\"\\nüîç Testing: {Path(test_dir).name}\")\n",
    "            \n",
    "            predictions = predict_one_fast(loaded_model, test_dir, batch_size=8)\n",
    "            if predictions:\n",
    "                lvef_results = calculate_lvef_from_predictions(predictions)\n",
    "                if lvef_results:\n",
    "                    print(f\"   Frames: {lvef_results['total_frames']}\")\n",
    "                    print(f\"   ESV: {lvef_results['ESV_pixel']} pixels\")\n",
    "                    print(f\"   EDV: {lvef_results['EDV_pixel']} pixels\")\n",
    "                    print(f\"   LVEF: {lvef_results['LVEF_pixel']:.2f}%\")\n",
    "\n",
    "# Uncomment to test single video first\n",
    "# test_single_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
